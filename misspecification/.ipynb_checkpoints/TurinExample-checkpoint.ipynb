{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script runs the HITL-ABC on the data-sets from Turin model. \n",
    "The data-sets are generated using Turin_generateData.R and TurinMisspecification.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy import stats\n",
    "import functions as f\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data-sets\n",
    "data_param = np.asarray(pyreadr.read_r('Turin_param_miss')[None])\n",
    "data_param = data_param[:,:3] # Parameters\n",
    "\n",
    "# Observed statistics after corruption\n",
    "# s_obs = np.asarray(pyreadr.read_r('Turin_sobs_zeta1')[None]).T # zeta = 1\n",
    "s_obs = np.asarray(pyreadr.read_r('Turin_sobs_zeta5')[None]).T # zeta = 5\n",
    "# s_obs = np.asarray(pyreadr.read_r('Turin_sobs_zeta10')[None]).T # zeta = 10\n",
    "\n",
    "data_sim = np.asarray(pyreadr.read_r('Turin_ssim_miss')[None])\n",
    "s_sim = data_sim[:,:6] # Simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "G0_start = 1e-10\n",
    "G0_end = 3e-9\n",
    "T_start = 1e-9\n",
    "T_end = 2e-8\n",
    "lambda_start = 1e8\n",
    "lambda_end = 4e9\n",
    "\n",
    "parameters = {\n",
    "    'G0':{\n",
    "        'display_name': '$G_0$',\n",
    "        'val': 1e-9\n",
    "        },\n",
    "    'T':{\n",
    "        'display_name': 'T',\n",
    "        'val': 1e-8\n",
    "        },\n",
    "    'lambda':{\n",
    "        'display_name': '$\\lambda$',\n",
    "        'val': 1e9\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Uniform prior specifications\n",
    "priors = {\n",
    "    'G0':{\n",
    "        'start': G0_start,\n",
    "        'stop': G0_end\n",
    "        },\n",
    "    'T':{\n",
    "        'start': T_start,\n",
    "        'stop': T_end\n",
    "        },\n",
    "    'lambda':{\n",
    "        'start': lambda_start,\n",
    "        'stop': lambda_end\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no.1, utilities:\n",
      "[0.34091135 0.28516855 0.26245663 0.46706563 0.3205958  0.27314045]\n",
      "The queried statistic is no.4\n",
      "Obtained feedback: 0\n",
      "\n",
      "Iteration no.2, utilities:\n",
      "[0.22031764 0.19786801 0.14488314 0.         0.14562931 0.1201123 ]\n",
      "The queried statistic is no.1\n",
      "Obtained feedback: 1\n",
      "\n",
      "Iteration no.3, utilities:\n",
      "[0.         0.2811417  0.17986837 0.         0.10181491 0.16829819]\n",
      "The queried statistic is no.2\n",
      "Obtained feedback: 1\n",
      "\n",
      "Iteration no.4, utilities:\n",
      "[0.         0.         0.11936822 0.         0.18377852 0.13063004]\n",
      "The queried statistic is no.5\n",
      "Obtained feedback: 1\n",
      "\n",
      "Iteration no.5, utilities:\n",
      "[0.         0.         0.10388221 0.         0.         0.03817821]\n",
      "The queried statistic is no.3\n",
      "Obtained feedback: 1\n",
      "\n",
      "Iteration no.6, utilities:\n",
      "[0.         0.         0.         0.         0.         0.06236539]\n",
      "The queried statistic is no.6\n",
      "Obtained feedback: 1\n",
      "\n",
      "Obtained gamma_hat: [1. 1. 1. 0. 1. 1.]\n",
      "Generating final ABC results...\n"
     ]
    }
   ],
   "source": [
    "M = 2000 # No. of samples from prior\n",
    "M_epsilon = 0.05 # Tolerance threshold\n",
    "nGSamples = 4000 # No. of samples used to estimate the KL divergence\n",
    "nStats = 6 # No. of statistics\n",
    "\n",
    "param = data_param[0:M, :]\n",
    "param = (param - np.mean(param, 0)) / np.std(param, axis = 0) # Standardizing the parameters for KL divergence computation\n",
    "\n",
    "# parameters\n",
    "rho = 0.5\n",
    "pi = 0.95\n",
    "delta = 0.06\n",
    "nParam = len(parameters)\n",
    "\n",
    "gamma_true = np.ones(nStats)\n",
    "gamma_true = [1,1,1,0,1,1]\n",
    "\n",
    "# Computing prior predictive probability of f\n",
    "predProb_f1 = rho * pi + (1 - rho) * (1 - pi)\n",
    "predProb_f0 = 1 - predProb_f1\n",
    "\n",
    "queried_idx = []  # Initializing an empty list that will contain the indices of queried statistics\n",
    "feedbacks = []  # Initializing an empy list that will contain the feedbacks\n",
    "\n",
    "k = 0\n",
    "d = 1\n",
    "while (k < nStats and d > delta):\n",
    "    # Initializing vector of utilities\n",
    "    utility = np.zeros(nStats)\n",
    "\n",
    "    gamma_vector_current = f.sample_gamma(queried_idx, feedbacks, nGSamples, nStats, pi, rho)\n",
    "    samples_current = f.sample_ABC_Posterior(s_obs, param, s_sim, M_epsilon, gamma_vector_current)\n",
    "\n",
    "    for j in range(nStats):\n",
    "        if j not in queried_idx:  # Skipping the utility computation if the jth statistic has already been queried\n",
    "            # Sample from p_ABC(theta | y, F, fj = 1)\n",
    "            gamma_vector_new1 = f.sample_gamma(queried_idx + [j], feedbacks + [1], nGSamples, nStats, pi, rho)\n",
    "            samples_future_f1 = f.sample_ABC_Posterior(s_obs, param, s_sim, M_epsilon, gamma_vector_new1)\n",
    "            \n",
    "            # Sample from p_ABC(theta | y, F, fj = 0)\n",
    "            gamma_vector_new0 = f.sample_gamma(queried_idx + [j], feedbacks + [0], nGSamples, nStats, pi, rho)\n",
    "            samples_future_f0 = f.sample_ABC_Posterior(s_obs, param, s_sim, M_epsilon, gamma_vector_new0)\n",
    "            # Computing KL divergence\n",
    "            KL_estimate_f1 = f.KLdivergence(samples_future_f1, samples_current)\n",
    "            KL_estimate_f0 = f.KLdivergence(samples_future_f0, samples_current)\n",
    "            # Computing utility\n",
    "            utility[j] = predProb_f1 * KL_estimate_f1 + predProb_f0 * KL_estimate_f0\n",
    "\n",
    "\n",
    "    print('Iteration no.' + str(k+1) + ', utilities:')\n",
    "    print(utility)\n",
    "    d = utility.max()\n",
    "    if d > delta:\n",
    "        print('The queried statistic is no.' + str(np.argmax(utility)+1))\n",
    "        queried_idx.append(np.argmax(utility))\n",
    "        feedbacks.append(gamma_true[np.argmax(utility)])\n",
    "        print('Obtained feedback: ' + str(feedbacks[-1]) + '\\n')\n",
    "    else:\n",
    "        print('We do not need to query another feedback from the expert! \\n')\n",
    "    k = k + 1\n",
    "\n",
    "gamma_hat = np.zeros(nStats)\n",
    "gamma_hat[queried_idx] = feedbacks\n",
    "print('Obtained gamma_hat: ' + str(gamma_hat))\n",
    "print('Generating final ABC results...')\n",
    "samples_final = f.regression_ABC(s_obs, param, s_sim, M_epsilon, gamma_hat.astype('bool'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
